{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h]\n",
      "                   [--classifiers {SAG,ExtraTrees,GBRT,SGD,liblinear,CART,RandomForest,GaussianNB} [{SAG,ExtraTrees,GBRT,SGD,liblinear,CART,RandomForest,GaussianNB} ...]]\n",
      "                   [--n-jobs [N_JOBS]] [--order [{F,C}]]\n",
      "                   [--random-seed [RANDOM_SEED]]\n",
      "__main__.py: error: unrecognized arguments: -f /Users/yunfanyang/Library/Jupyter/runtime/kernel-80b91648-45db-475f-b68d-ed5fc6e8e776.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===========================\n",
    "Covertype dataset benchmark\n",
    "===========================\n",
    "\n",
    "Benchmark stochastic gradient descent (SGD), Liblinear, and Naive Bayes, CART\n",
    "(decision tree), RandomForest and Extra-Trees on the forest covertype dataset\n",
    "of Blackard, Jock, and Dean [1]. The dataset comprises 581,012 samples. It is\n",
    "low dimensional with 54 features and a sparsity of approx. 23%. Here, we\n",
    "consider the task of predicting class 1 (spruce/fir). The classification\n",
    "performance of SGD is competitive with Liblinear while being two orders of\n",
    "magnitude faster to train::\n",
    "\n",
    "    [..]\n",
    "    Classification performance:\n",
    "    ===========================\n",
    "    Classifier   train-time test-time error-rate\n",
    "    --------------------------------------------\n",
    "    liblinear     15.9744s    0.0705s     0.2305\n",
    "    GaussianNB    3.0666s     0.3884s     0.4841\n",
    "    SGD           1.0558s     0.1152s     0.2300\n",
    "    CART          79.4296s    0.0523s     0.0469\n",
    "    RandomForest  1190.1620s  0.5881s     0.0243\n",
    "    ExtraTrees    640.3194s   0.6495s     0.0198\n",
    "\n",
    "The same task has been used in a number of papers including:\n",
    "\n",
    " * `\"SVM Optimization: Inverse Dependence on Training Set Size\"\n",
    "   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.2112>`_\n",
    "   S. Shalev-Shwartz, N. Srebro - In Proceedings of ICML '08.\n",
    "\n",
    " * `\"Pegasos: Primal estimated sub-gradient solver for svm\"\n",
    "   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513>`_\n",
    "   S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07.\n",
    "\n",
    " * `\"Training Linear SVMs in Linear Time\"\n",
    "   <www.cs.cornell.edu/People/tj/publications/joachims_06a.pdf>`_\n",
    "   T. Joachims - In SIGKDD '06\n",
    "\n",
    "[1] http://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division, print_function\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Arnaud Joly <arnaud.v.joly@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_covtype, get_data_home\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "# Memoize the data extraction and memory map the resulting\n",
    "# train / test splits in readonly mode\n",
    "memory = Memory(os.path.join(get_data_home(), 'covertype_benchmark_data'),\n",
    "                mmap_mode='r')\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def load_data(dtype=np.float32, order='C', random_state=13):\n",
    "    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n",
    "    ######################################################################\n",
    "    ## Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    data = fetch_covtype(download_if_missing=True, shuffle=True,\n",
    "                         random_state=random_state)\n",
    "    X = check_array(data['data'], dtype=dtype, order=order)\n",
    "    y = (data['target'] != 1).astype(np.int)\n",
    "\n",
    "    ## Create train-test split (as [Joachims, 2006])\n",
    "    print(\"Creating train-test split...\")\n",
    "    n_train = 522911\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "\n",
    "    ## Standardize first 10 features (the numerical ones)\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    mean[10:] = 0.0\n",
    "    std[10:] = 1.0\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "ESTIMATORS = {\n",
    "    'GBRT': GradientBoostingClassifier(n_estimators=250),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=20),\n",
    "    'CART': DecisionTreeClassifier(min_samples_split=5),\n",
    "    'SGD': SGDClassifier(alpha=0.001, n_iter=2),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'liblinear': LinearSVC(loss=\"l2\", penalty=\"l2\", C=1000, dual=False,\n",
    "                           tol=1e-3),\n",
    "    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--classifiers', nargs=\"+\",\n",
    "                        choices=ESTIMATORS, type=str,\n",
    "                        default=['liblinear', 'GaussianNB', 'SGD', 'CART'],\n",
    "                        help=\"list of classifiers to benchmark.\")\n",
    "    parser.add_argument('--n-jobs', nargs=\"?\", default=1, type=int,\n",
    "                        help=\"Number of concurrently running workers for \"\n",
    "                             \"models that support parallelism.\")\n",
    "    parser.add_argument('--order', nargs=\"?\", default=\"C\", type=str,\n",
    "                        choices=[\"F\", \"C\"],\n",
    "                        help=\"Allow to choose between fortran and C ordered \"\n",
    "                             \"data\")\n",
    "    parser.add_argument('--random-seed', nargs=\"?\", default=13, type=int,\n",
    "                        help=\"Common seed used by random number generator.\")\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    print(__doc__)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = load_data(\n",
    "        order=args[\"order\"], random_state=args[\"random_seed\"])\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(\"===================\")\n",
    "    print(\"%s %d\" % (\"number of features:\".ljust(25), X_train.shape[1]))\n",
    "    print(\"%s %d\" % (\"number of classes:\".ljust(25), np.unique(y_train).size))\n",
    "    print(\"%s %s\" % (\"data type:\".ljust(25), X_train.dtype))\n",
    "    print(\"%s %d (pos=%d, neg=%d, size=%dMB)\"\n",
    "          % (\"number of train samples:\".ljust(25),\n",
    "             X_train.shape[0], np.sum(y_train == 1),\n",
    "             np.sum(y_train == 0), int(X_train.nbytes / 1e6)))\n",
    "    print(\"%s %d (pos=%d, neg=%d, size=%dMB)\"\n",
    "          % (\"number of test samples:\".ljust(25),\n",
    "             X_test.shape[0], np.sum(y_test == 1),\n",
    "             np.sum(y_test == 0), int(X_test.nbytes / 1e6)))\n",
    "\n",
    "    print()\n",
    "    print(\"Training Classifiers\")\n",
    "    print(\"====================\")\n",
    "    error, train_time, test_time = {}, {}, {}\n",
    "    for name in sorted(args[\"classifiers\"]):\n",
    "        print(\"Training %s ... \" % name, end=\"\")\n",
    "        estimator = ESTIMATORS[name]\n",
    "        estimator_params = estimator.get_params()\n",
    "\n",
    "        estimator.set_params(**{p: args[\"random_seed\"]\n",
    "                                for p in estimator_params\n",
    "                                if p.endswith(\"random_state\")})\n",
    "\n",
    "        if \"n_jobs\" in estimator_params:\n",
    "            estimator.set_params(n_jobs=args[\"n_jobs\"])\n",
    "\n",
    "        time_start = time()\n",
    "        estimator.fit(X_train, y_train)\n",
    "        train_time[name] = time() - time_start\n",
    "\n",
    "        time_start = time()\n",
    "        y_pred = estimator.predict(X_test)\n",
    "        test_time[name] = time() - time_start\n",
    "\n",
    "        error[name] = zero_one_loss(y_test, y_pred)\n",
    "\n",
    "        print(\"done\")\n",
    "\n",
    "    print()\n",
    "    print(\"Classification performance:\")\n",
    "    print(\"===========================\")\n",
    "    print(\"%s %s %s %s\"\n",
    "          % (\"Classifier  \", \"train-time\", \"test-time\", \"error-rate\"))\n",
    "    print(\"-\" * 44)\n",
    "    for name in sorted(args[\"classifiers\"], key=error.get):\n",
    "        print(\"%s %s %s %s\" % (name.ljust(12),\n",
    "                               (\"%.4fs\" % train_time[name]).center(10),\n",
    "                               (\"%.4fs\" % test_time[name]).center(10),\n",
    "                               (\"%.4f\" % error[name]).center(10)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
